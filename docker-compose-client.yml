version: "3.9"

services:
  client:
    command: ["flask", "run", "--host", "0.0.0.0", "--port", "5000"]
    build:
      dockerfile: client.Dockerfile
      context: .
    ports:
      - "50050:5000"
    # Reuse your huggingface cache to avoid downloading models again
    environment:
      - HF_DATASETS_CACHE=/app/models/cache
      - LLM_HOST=devgrt.aivillage.org
      - LLM_PORT=50055
      - LLM_API_KEY=eeb420b9-2832-473c-b560-64b12523d718 # change this to the actual key if needed
    volumes:
      - ~/.cache/huggingface/hub:/app/models/cache
    restart: always

  web:
    build:
      dockerfile: Dockerfile
      context: ./web
    ports:
      - "50051:3000"
    depends_on:
      - client
    restart: always